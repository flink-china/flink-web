<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="description" content="Apache Flink是一款为分布式、高性能、高可用、高精确的数据流应用而生的开源流式处理框架。">
    <meta name="keywords" content="flink,flink china,flink中文,流计算,流式处理框架,大数据学习">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink: 常见问题(FAQ)</title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/flink.css">
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Blog RSS feed -->
    <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Apache Flink Blog: RSS feed" />

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!-- We need to load Jquery in the header for custom google analytics event tracking-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>

    <!-- 百度统计 -->
    <script>
      var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?0313b97a11c5a890f310b220f89b56cf";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  <meta name="baidu-site-verification" content="wfuLdxO06m" />
  </head>
  <body>
    

    <!-- Main content. -->
    <div class="container">
    <div class="row">

      
     <div id="sidebar" class="col-sm-3">
          <!-- Top navbar. -->
    <nav class="navbar navbar-default">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="/">
              <img alt="Apache Flink" src="/img/flink-header-logo.svg" width="147px" height="73px">
            </a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-main">

            <!-- Downloads -->
            <li class=""><a class="btn btn-info" href="http://flink.apache.org/downloads.html">下载 Flink</a></li>

            <!-- Overview -->
            <li><a href="/index.html">首页</a></li>

            <!-- Intro -->
            <li><a href="/introduction.html">Flink 介绍</a></li>

            <!-- Use cases -->
            <li><a href="/usecases.html">Flink 用户案例</a></li>

            <!-- Powered by -->
            <li><a href="/poweredby.html">由Flink提供支持</a></li>

            <!-- Ecosystem -->
            <li><a href="/ecosystem.html">生态系统</a></li>

            <!-- Community -->
            <li><a href="/community.html">社区和项目</a></li>

            <!-- Contribute -->
            <li><a href="/how-to-contribute.html">如何贡献</a></li>


            <hr />



            <!-- Documentation -->
            <li>
              <a href="http://doc.flink-china.org/latest/" target="_blank">中文文档 <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>

            <!-- <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">中文文档
                <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="http://doc.flink-china.org/latest/" target="_blank">1.3 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                  <li><a href="http://ci.apache.org/projects/flink/flink-docs-master" target="_blank">1.4 (Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                </ul>
              </li> -->

            <!-- Quickstart -->
            <li>
              <a href="http://doc.flink-china.org/latest//quickstart/setup_quickstart.html" target="_blank">快速起步 <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>
            <!--Flink学院暂时注释掉
              <li><a href="/school.html">Flink学院</a></li>

              <li><a href="/active.html">Flink China社区活动</a></li>
           </li> -->


          <ul class="nav navbar-nav navbar-bottom">
          <hr />

            <!-- FAQ -->

            <li  class="hidden-sm active"><a href="/faq.html">常见问题</a></li>
            <li ><a href="/translation-team.html">Flink China翻译团队</a></li>

          </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

      </div>
      <div class="col-sm-9">
      <div class="row-fluid">
  <div class="col-sm-12">
    <h1>常见问题(FAQ)</h1>

	<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

<p>以下是关于Flink项目的常见问题。如果你还有其他问题，可以<a href="http://flink-china.org/faq.html">参阅文档</a>或询问<a href="http://flink-china.org/faq.html">社区</a>。</p>

<div class="page-toc">
<ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">综述</a>    <ul>
      <li><a href="#flinkhadoop" id="markdown-toc-flinkhadoop">Flink是一个Hadoop项目吗？</a></li>
      <li><a href="#apache-hadoopflink" id="markdown-toc-apache-hadoopflink">我需要安装Apache Hadoop来使用Flink吗?</a></li>
    </ul>
  </li>
  <li><a href="#section-1" id="markdown-toc-section-1">用法</a>    <ul>
      <li><a href="#flink" id="markdown-toc-flink">如何评估Flink计划的进展？</a></li>
      <li><a href="#section-2" id="markdown-toc-section-2">怎样才能找出程序失败的原因？</a></li>
      <li><a href="#flink-1" id="markdown-toc-flink-1">如何调试Flink程序？</a></li>
      <li><a href="#section-3" id="markdown-toc-section-3">平行度是什么？如何设置它？</a></li>
    </ul>
  </li>
  <li><a href="#section-4" id="markdown-toc-section-4">错误</a>    <ul>
      <li><a href="#nonserializableexception" id="markdown-toc-nonserializableexception">为什么我会收到“NonSerializableException”？</a></li>
      <li><a href="#scala-api" id="markdown-toc-scala-api">在Scala API中，我收到了关于隐式值和证据参数的报错</a></li>
      <li><a href="#section-5" id="markdown-toc-section-5">我收到一条错误消息，说没有足够的缓冲区可用。我怎么解决这个问题？</a></li>
      <li><a href="#javaioeof" id="markdown-toc-javaioeof">我的作业在早期就因java.io.EOF异常失败。可能的原因是什么？</a></li>
      <li><a href="#hdfshadoop" id="markdown-toc-hdfshadoop">我的作业因为HDFS/Hadoop代码的各种异常失败。我能做什么?</a></li>
      <li><a href="#eclipsescala" id="markdown-toc-eclipsescala">Eclipse开发中，Scala项目中的编译错误</a></li>
      <li><a href="#keysgroupedjoined" id="markdown-toc-keysgroupedjoined">我的程序没有计算出正确的结果。为什么我自定义keys没有被正确地grouped/joined？</a></li>
      <li><a href="#javalanginstantiation" id="markdown-toc-javalanginstantiation">我的数据类型发生java.lang.Instantiation异常，哪里出错了？</a></li>
      <li><a href="#flink-2" id="markdown-toc-flink-2">我用停止脚本停不掉Flink，怎么办？</a></li>
      <li><a href="#outofmemory" id="markdown-toc-outofmemory">我收到了OutOfMemory异常，该怎么处理？</a></li>
      <li><a href="#section-6" id="markdown-toc-section-6">任务管理器的日志文件为什么变得如此巨大？</a></li>
      <li><a href="#section-7" id="markdown-toc-section-7">分配给我的任务管理器的槽已经被释放了。我应该做什么?</a></li>
    </ul>
  </li>
  <li><a href="#yarn" id="markdown-toc-yarn">YARN开发</a>    <ul>
      <li><a href="#yarn-1" id="markdown-toc-yarn-1">YARN会话只持续了数秒</a></li>
      <li><a href="#yarnkill" id="markdown-toc-yarnkill">YARN容器由于消耗太多内存而被kill</a></li>
      <li><a href="#hdfsyarn" id="markdown-toc-hdfsyarn">启动过程中HDFS许可异常引起的YARN会话崩溃</a></li>
      <li><a href="#section-8" id="markdown-toc-section-8">我的作业取消为什么没有反应?</a></li>
    </ul>
  </li>
  <li><a href="#section-9" id="markdown-toc-section-9">产品特点</a>    <ul>
      <li><a href="#flink-3" id="markdown-toc-flink-3">Flink提供什么样的容错系统?</a></li>
      <li><a href="#flinkhadoop-1" id="markdown-toc-flinkhadoop-1">Flink是否支持Hadoop式计数器和分布式缓存？</a></li>
    </ul>
  </li>
</ul>

</div>

<h2 id="section">综述</h2>

<h3 id="flinkhadoop">Flink是一个Hadoop项目吗？</h3>

<p>Flink是一个数据处理系统，是<strong>Hadoop的MapReduce组件的替代品</strong>。它有自己运行环境，而不是建立在MapReduce上。因此，它可以完全独立于Hadoop生态系统。尽管如此，Flink依然可以接入Hadoop的分布式文件系统(HDFS)来读取和写入数据，也可以使用Hadoop下一代资源调度系统（YARN）提供集群资源调度。由于绝大多数Flink用户都使用Hadoop HDFS来存储数据，Flink已经将所需的库接入到HDFS上。</p>

<h3 id="apache-hadoopflink">我需要安装Apache Hadoop来使用Flink吗?</h3>

<p><strong>不是</strong>。Flink可以在<strong>没有</strong>Hadoop的环境下运行。然而，通常的设置是使用Flink分析存储在Hadoop Distributed File System（HDFS）上的数据。为了使这些设置能够正常工作，Flink在默认情况下集成了Hadoop客户端。
此外，我们为现有的Hadoop YARN集群提供了特别的YARN Flink下载版本。 <a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop
YARN</a>
是Hadoop的集群资源管理器，它允许在集群上使用不同的执行引擎。</p>

<h2 id="section-1">用法</h2>

<h3 id="flink">如何评估Flink计划的进展？</h3>

<p>有多种方法可以跟踪Flink计划的进展情况：</p>

<ul>
  <li>JobManager（分布式系统的主设备）启动Web界面以观察程序执行。 在默认情况下在端口8081上运行（在conf / flink-config.yml中配置）。</li>
  <li>从命令行启动程序时，随着程序在操作中的进展，它将打印所有操作员的状态更改。</li>
  <li>所有状态更改也会记录到JobManager的日志文件中。</li>
</ul>

<h3 id="section-2">怎样才能找出程序失败的原因？</h3>

<ul>
  <li>JobManager Web前端（默认情况下在端口8081上）显示失败任务的例外情况。</li>
  <li>如果从命令行运行程序，任务例外将打印到标准错误流并显示在控制台上。</li>
  <li>通过命令行和Web界面，您可以确定哪个并行任务首先失败，并导致其他任务取消执行。</li>
  <li>发生异常的主服务器和工作服务器的日志文件(<code>log/flink-&lt;user&gt;-jobmanager-&lt;host&gt;.log</code> 和<code>log/flink-&lt;user&gt;-taskmanager-&lt;host&gt;.log</code>).</li>
</ul>

<h3 id="flink-1">如何调试Flink程序？</h3>

<ul>
  <li>当您使用<a href="http://ci.apache.org/projects/flink/flink-docs-master/apis/local_execution.html">LocalExecutor</a>在本地启动程序时，可以在函数中放置断点并像普通的Java / Scala程序一样调试它们。</li>
  <li><a href="http://ci.apache.org/projects/flink/flink-docs-master/apis/programming_guide.html#accumulators--counters">Accumulators</a> 在跟踪并行执行的行为方面非常有用。它们允许您收集程序操作中的信息并在程序执行后显示它们。</li>
</ul>

<h3 id="section-3">平行度是什么？如何设置它？</h3>

<p>在Flink程序中，并行性决定了操作如何分解为分配给任务槽的单个任务。 群集中的每个节点至少有一个任务槽。 任务槽的总数是所有机器上所有任务槽的数量。 如果并行性设置为<code>N</code>, 则Flink会尝试将操作划分为 <code>N</code> 个并行任务，这些任务可以使用可用任务槽同时计算。 任务槽的数量应该等于并行度，以确保所有任务可以同时在一个任务槽中计算。</p>

<p>注意: 并非所有操作都可以分为多个任务。 例如，没有分组的
<code>GroupReduce</code> 操作必须以1的并行性执行，因为整个组需要存在于恰好一个节点以执行减少操作。link将确定并行性是否为1并相应地设置它。
并行性可以通过多种方式进行设置，以确保对Flink程序的执行进行精细控制。 有关如何设置并行性的详细说明，请参阅查阅 <a href="http://ci.apache.org/projects/flink/flink-docs-master/setup/config.html#common-options">配置指南</a> 。 另外请查看 <a href="http://ci.apache.org/projects/flink/flink-docs-master/setup/config.html#configuring-taskmanager-processing-slots">this figure</a> 详细说明处理插槽和并行性是如何相互关联的。</p>

<h2 id="section-4">错误</h2>

<h3 id="nonserializableexception">为什么我会收到“NonSerializableException”？</h3>

<p>Flink中的所有函数必须是序列化的，这是由 <a href="http://docs.oracle.com/javase/7/docs/api/java/io/Serializable.html">java.io.Serializable</a>定义的。.
由于所有的程序接口都是可序列化的，异常意味着您的函数中使用的一个域不是可序列化的。</p>

<p>特别地，如果你的函数是一个内部类，或者匿名内部类，它包含一个隐藏的对封闭类的引用。（通常叫做this$0，如果你看一下调试器中的函数）如果封闭类不是可序列化的，   那么这可能就是错误的根源。解决方案：</p>

<ul>
  <li>使该函数成为一个独立的类，或者一个静态内部类（不再引用封闭类）</li>
  <li>使封闭类可串行化</li>
  <li>使用Java 8 lambda函数</li>
</ul>

<h3 id="scala-api">在Scala API中，我收到了关于隐式值和证据参数的报错</h3>

<p>这意味着不能提供类型信息的隐式值。确保在你的代码中有 <code>import org.apache.flink.api.scala._</code> 的声明.</p>

<p>如果你在使用通用参数的函数或类中使用Flink操作，那么必须为该参数提供类型定义。可以通过上下文邦定来实现：</p>

<div class="highlight"><pre><code class="language-scala"><span class="k">def</span> <span class="n">myFunction</span><span class="o">[</span><span class="kt">T:</span> <span class="kt">TypeInformation</span><span class="o">](</span><span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="n">input</span><span class="o">.</span><span class="n">reduceGroup</span><span class="o">(</span> <span class="n">i</span> <span class="k">=&gt;</span> <span class="n">i</span><span class="o">.</span><span class="n">toSeq</span> <span class="o">)</span>
<span class="o">}</span></code></pre></div>

<p>参阅 <a href="http://ci.apache.org/projects/flink/flink-docs-master/internals/types_serialization.html">类型提取和序列化</a> 了解关于Flink如何处理类型的深入讨论。</p>

<h3 id="section-5">我收到一条错误消息，说没有足够的缓冲区可用。我怎么解决这个问题？</h3>

<p>如果您在一个大规模并发环境（100多个并行线程）中运行Flink，您需要通过配置参数
<code>taskmanager.network.numberOfBuffers</code>.
调整网络缓冲区的数量。根据经验法则，缓冲区的数量应该是最少的
<code>4 * numberOfTaskManagers * numberOfSlotsPerTaskManager^2</code>. 查阅
<a href="http://ci.apache.org/projects/flink/flink-docs-master/setup/config.html#configuring-the-network-buffers">Configuration Reference</a> 获取更多细节.</p>

<h3 id="javaioeof">我的作业在早期就因java.io.EOF异常失败。可能的原因是什么？</h3>

<p>对于这些异常，最常见的情况是当Flink设置时对应错了HDFS的版本。不同版本的HDFS经常不兼容，导致文件系统主服务器和客户机之间的连接中断。</p>

<div class="highlight"><pre><code class="language-bash">Call to &lt;host:port&gt; failed on <span class="nb">local </span>exception: java.io.EOFException
    at org.apache.hadoop.ipc.Client.wrapException<span class="o">(</span>Client.java:775<span class="o">)</span>
    at org.apache.hadoop.ipc.Client.call<span class="o">(</span>Client.java:743<span class="o">)</span>
    at org.apache.hadoop.ipc.RPC<span class="nv">$Invoker</span>.invoke<span class="o">(</span>RPC.java:220<span class="o">)</span>
    at <span class="nv">$Proxy0</span>.getProtocolVersion<span class="o">(</span>Unknown Source<span class="o">)</span>
    at org.apache.hadoop.ipc.RPC.getProxy<span class="o">(</span>RPC.java:359<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode<span class="o">(</span>DFSClient.java:106<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;<span class="o">(</span>DFSClient.java:207<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;<span class="o">(</span>DFSClient.java:170<span class="o">)</span>
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize<span class="o">(</span>DistributedFileSystem.java:82<span class="o">)</span>
    at org.apache.flinkruntime.fs.hdfs.DistributedFileSystem.initialize<span class="o">(</span>DistributedFileSystem.java:276</code></pre></div>

<p>有关如何为不同的Hadoop和HDFS版本设置Flink的详细信息，请参考 <a href="/downloads.html#maven">下载页面</a> 和 <a href="https://github.com/apache/flink/tree/master/README.md">构建说明</a></p>

<h3 id="hdfshadoop">我的作业因为HDFS/Hadoop代码的各种异常失败。我能做什么?</h3>

<p>默认情况下，Flink将附带Hadoop 2.2版本的二进制文件。这些二进制文件用于连接HDFS或YARN。HDFS客户端似乎有一些bug，在写入HDFS时（特别是在高负载下）会导致异常。以下是异常的情况：</p>

<ul>
  <li><code>HDFS client trying to connect to the standby Namenode "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby"</code></li>
  <li>
    <p><code>java.io.IOException: Bad response ERROR for block BP-1335380477-172.22.5.37-1424696786673:blk_1107843111_34301064 from datanode 172.22.5.81:50010
at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:732)</code></p>
  </li>
  <li><code>Caused by: org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 0
      at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeManager.java:478)
      at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipelineInternal(FSNamesystem.java:6039)
      at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipeline(FSNamesystem.java:6002)</code></li>
</ul>

<p>如果您经历了以上任何一种，我们建议使用一个与您的本地HDFS版本相匹配的Hadoop版本的Flink构建。您还可以根据Hadoop版本（例如使用自定义补丁级别的Hadoop）手动构建Flink。</p>

<h3 id="eclipsescala">Eclipse开发中，Scala项目中的编译错误</h3>

<p>Flink使用了Scala编译器的一个新特性（称为“准引号”），它还没有与Eclipse Scala插件进行很好的集成。为了使这个特性在Eclipse中可用，您需要手动配置Flink-Scala项目来使用编译器插件：</p>

<ul>
  <li>右键点击“Flink-Scala”并选择“属性”；</li>
  <li>选择 “Scala Compiler” 并点击 “Advanced” 选项卡. 如果没有，您可能没有为Scala设置合适的Eclipse）</li>
  <li>检查框“”Use Project Settings”</li>
  <li>在 “Xplugin”字段中, 将路径 “/home/<user-name>/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar"
注意：您必须首先在命令行上与Maven构建Flink，以确保插件被下载。</user-name></li>
</ul>

<h3 id="keysgroupedjoined">我的程序没有计算出正确的结果。为什么我自定义keys没有被正确地grouped/joined？</h3>

<p>Keys必须正确地实现 <code>java.lang.Object#hashCode()</code>,
<code>java.lang.Object#equals(Object o)</code>, 和 <code>java.util.Comparable#compareTo(...)</code>.
这些方法总是支持默认的实现，而这些实现通常是不充分的。因此，所有的键必须覆盖<code>hashCode()</code>和<code>equals(Object o)</code>。</p>

<h3 id="javalanginstantiation">我的数据类型发生java.lang.Instantiation异常，哪里出错了？</h3>

<p>所有的数据类型类都必须是公有的，并且有一个公共的nullary构造函数（没有参数的构造函数）。此外，类不能是抽象的或接口。如果类是内部类，它们必须是公有的和静态的。</p>

<h3 id="flink-2">我用停止脚本停不掉Flink，怎么办？</h3>

<p>停止进程有时需要几秒钟，因为停止可能会进行一些清理工作。
在一些错误情况下用提供的停止脚本(<code>bin/stop-local.sh</code> or <code>bin/stop-
cluster.sh</code>)停不掉作业管理器或者任务管理器。你可以在linux/mac上kill它们的进程：</p>

<ul>
  <li>
    <p>确定作业管理器或者任务管理器进程的进程id（pid），你可以在Linux上用 <code>jps</code>命令（如果OpenSDK已经安装）或者<code>ps -ef | grep java</code>命令找到所有的java进程。</p>
  </li>
  <li>
    <p>用<code>kill -9 &lt;pid&gt;</code>命令结束掉受影响的JobManager或者TaskManager的<code>pid</code>。
在Windows系统上，任务管理器列出了所有进程的表，并允许您通过他的入口来销毁进程。</p>
  </li>
</ul>

<p>在Windows系统上，任务管理器列出了所有进程的表，并允许您通过他的入口来销毁进程。</p>

<p>作业管理器和任务管理器服务都将把信号（如SIGKILL和SIGTERM）写入各自的日志文件中。这对于调试停止问题很有帮助</p>

<h3 id="outofmemory">我收到了OutOfMemory异常，该怎么处理？</h3>

<p>这些异常通常发生在程序中的函数消耗大量内存来收集对象，例如列表或图表。Java中的OutOfMemory异常有点棘手。这个异常并不一定是由分配了太多内存组件引起，而是由组件试图响应的最新的内存无法得到释放。</p>

<p>有两种解决方法：</p>

<ol>
  <li>
    <p>您是否可以在函数中使用更少的内存。例如，使用原始类型的数组而不是对象类型。</p>
  </li>
  <li>
    <p>减少Flink为其自身处理而保留的内存。任务管理器保留了可用内存的一部分，用于排序、散列、缓存、网络缓冲等。用户定义函数无法访问该部分内存。通过保留它，系统可以保证在大量输入不会耗尽内存，但是如果需要，可以使用可用内存和destage操作来进行划分。默认情况下，系统会保留大约70%的内存。如果您经常在udf中运行需要更多内存的应用程序，您可以使用<code>taskmanager.memory.fraction</code> 或者<code>taskmanager.memory.size</code>来减少该值。参阅<a href="http://ci.apache.org/projects/flink/flink-docs-master/setup/config.html">Configuration Reference</a>获取更多细节。这将为JVM堆留下更多内存，但可能会导致数据处理任务更频繁地进入磁盘。</p>
  </li>
</ol>

<p>OutOfMemoryExceptions的另一个原因是使用了错误的状态后端。默认情况下，Flink在流作业中使用基于堆的状态后端。RocksDBStateBackend允许状态尺寸大于可获得的堆空间。</p>

<h3 id="section-6">任务管理器的日志文件为什么变得如此巨大？</h3>

<p>检查您的作业的日志记录行为。每个对象或元组的日志记录可能有助于在小的数据集安装中来调试作业，但是如果用于大的输入数据会限制性能并消耗大量的磁盘空间。
分配给我的任务管理器的槽已经被释放了。我应该做什么?</p>

<h3 id="section-7">分配给我的任务管理器的槽已经被释放了。我应该做什么?</h3>

<p>如果你看到java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager 异常，即使任务管理器没有崩溃，它意味着任务管理器已经有一段时间没有响应。这可能是由于网络问题造成的，但通常是由于长期的垃圾收集摊位。在这种情况下，快速解决方案将是使用增量垃圾收集器，如G1垃圾收集器。它通常会带来更短的暂停。此外，您可以通过减少对其内部操作的内存占用（参见任务管理器托管内存的配置）来为用户代码提供更多的内存。</p>

<p>如果这两种方法都失败了，并且错误仍然存在，只需通过设置AKKA_WATCH_HEARTBEAT_PAUSE (akka.watch.heartbeat.pause)到更大的值（例如600秒），就可以增加TaskManager的脉搏暂停。这将导致作业管理器在考虑丢失之前等待一个更长的时间间隔。</p>

<h2 id="yarn">YARN开发</h2>

<h3 id="yarn-1">YARN会话只持续了数秒</h3>

<p>./bin/yarn-session.sh脚本在YARN会话开始时运行。在一些错误情况下，脚本立即停止执行。类似结果如下：</p>

<div class="highlight"><pre><code>07:34:27,004 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1395604279745_273123 to ResourceManager at jobtracker-host
Flink JobManager is now running on worker1:6123
JobManager Web Interface: http://jobtracker-host:54311/proxy/application_1295604279745_273123/
07:34:51,528 INFO  org.apache.flinkyarn.Client                                   - Application application_1295604279745_273123 finished with state FINISHED at 1398152089553
07:34:51,529 INFO  org.apache.flinkyarn.Client                                   - Killing the Flink-YARN application.
07:34:51,529 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Killing application application_1295604279745_273123
07:34:51,534 INFO  org.apache.flinkyarn.Client                                   - Deleting files in hdfs://user/marcus/.flink/application_1295604279745_273123
07:34:51,559 INFO  org.apache.flinkyarn.Client                                   - YARN Client is shutting down
</code></pre></div>

<p>这里的问题是ApplicationMaster（AM）停止了，而YARN客户端却认为AM已经完成。</p>

<p>这种情况有三种可能的原因：</p>

<ul>
  <li>
    <p>AM由于异常而退出。调试异常要查看容器的日志文件。yarn-site.xml文件包含已配置的路径。路径的key值为yarn.nodemanager.log-dirs，缺省值是${yarn.log.dir}/userlogs</p>
  </li>
  <li>
    <p>YARN已经结束了AM的容器。当AM使用过多的内存或其他资源超出了YARN的限制时，就会发生这种情况。在这种情况下，主机上的节点管理日志中会保存错误信息。</p>
  </li>
  <li>
    <p>T	操作系统关闭AM的JVM。如果YARN配置错误并且配置超过物理内存，就会发生这种情况。在AM运行的机器上执行dmesg可以查看是否发生这种情况。您可以从Linux的<a href="http://linux-mm.org/OOM_Killer">OOM killer</a>.</p>
  </li>
</ul>

<h3 id="yarnkill">YARN容器由于消耗太多内存而被kill</h3>

<p>通常会显示类似下面的一个log信息：</p>

<div class="highlight"><pre><code>Container container_e05_1467433388200_0136_01_000002 is completed with diagnostics: Container [pid=5832,containerID=container_e05_1467433388200_0136_01_000002] is running beyond physical memory limits. Current usage: 2.3 GB of 2 GB physical memory used; 6.1 GB of 4.2 GB virtual memory used. Killing container.
</code></pre></div>

<p>这种情况下，JVM进程太大。由于Java堆大小固定，额外内存来自于非堆源：</p>

<ul>
  <li>使用非堆内存的库。（Flink自身的堆外内存是有限的，并在计算堆允许大小时考虑在内）</li>
  <li>PermGen空间（字符串和类）、代码缓存、内存映射jar文件。</li>
  <li>本地库(RocksDB)</li>
</ul>

<p>You can activate the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.0/setup/config.html#memory-and-performance-debugging">memory debug logger</a> to get more insight into what memory pool is actually using up too much memory.</p>

<h3 id="hdfsyarn">启动过程中HDFS许可异常引起的YARN会话崩溃</h3>

<p>在开始YARN会话时，您会收到这样的异常：</p>

<div class="highlight"><pre><code>Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=robert, access=WRITE, inode="/user/robert":hdfs:supergroup:drwxr-xr-x
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:234)
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:214)
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:158)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5193)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5175)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5149)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2090)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2043)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1996)
  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:491)
  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:301)
  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59570)
  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2053)
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:396)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)

  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
  at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
  at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
  at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2021)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1989)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1954)
  at org.apache.flinkyarn.Utils.setupLocalResource(Utils.java:176)
  at org.apache.flinkyarn.Client.run(Client.java:362)
  at org.apache.flinkyarn.Client.main(Client.java:568)
</code></pre></div>
<p>出现这种错误的原因是，<strong>in HDFS</strong>中用户的主目录权限错误。用户(这个例子中的<code>robert</code>）不能在主目录下创建子目录。</p>

<p>Flink在用户主目录中创建一个<code>.flink/</code>目录，其中存储Flink jar和配置文件。</p>

<h3 id="section-8">我的作业取消为什么没有反应?</h3>

<p>Flink通过在所有用户任务上调用 <code>cancel()</code>方法来取消作业。理想情况下，任务对调用进行正确的响应，停止当前运行，这样所有线程都将关闭。</p>

<p>如果任务在一定的时间内没有反应，Flink将会周期性地中断线程。</p>

<p>TaskManager日志还包含用户代码被阻塞时调用方法的堆栈。</p>

<h2 id="section-9">产品特点</h2>

<h3 id="flink-3">Flink提供什么样的容错系统?</h3>

<p>对于流式计算来说，Flink采用新方法来绘制流数据状态的周期性快照，并使用它们进行恢复。这种机制高效灵活。 参阅 <a href="http://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html">streaming fault tolerance</a> 获得更多细节。</p>

<p>对于批量计算，Flink会记住程序的转换序列，并可以重启失败的作业。</p>

<h3 id="flinkhadoop-1">Flink是否支持Hadoop式计数器和分布式缓存？</h3>

<p><a href="http://ci.apache.org/projects/flink/flink-docs-master/apis/programming_guide.html#accumulators--counters">Flink的存储池</a> 和
Hadoop的计算器相比工作很类似, 但比后者更为强大.</p>

<p>Flink 的<a href="https://github.com/apache/flink/tree/master/flink-core/src/main/java/org/apache/flink/api/common/cache/DistributedCache.java">分布式缓存</a> 可以与API深度集成。 请查阅 <a href="https://github.com/apache/flink/tree/master/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java#L831">JavaDocs</a> 获取更多关于应用的细节来了解如何使用它。</p>

<p>为了使数据集在所有任务上可用，我们鼓励您使用 <a href="http://ci.apache.org/projects/flink/flink-docs-master/apis/programming_guide.html#broadcast-variables">Broadcast Variables</a> instead. 它们比分布式缓存更高效、简单。</p>


  </div>
</div>

      </div>
    </div>

    <hr />

    <div class="row">
      <div class="footer text-center col-sm-12">
        <p>Copyright © 2014-2017 <a href="http://apache.org">The Apache Software Foundation</a>. All Rights Reserved.</p>
        <p>Apache Flink, Flink®, Apache®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.</p>
        <p><a href="/privacy-policy.html">Privacy Policy</a> &middot; <a href="/blog/feed.xml">RSS feed</a></p>
      </div>
    </div>
    </div><!-- /.container -->

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="/js/codetabs.js"></script>
    <script src="/js/stickysidebar.js"></script>


    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
